{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du9Gy0IoJU7P"
      },
      "source": [
        "# Task 1: Multi layer perceptron\n",
        "\n",
        "In this task you will implement a multilayer perceptron which consists of multiple fully connected layers with biases each followed (potentially) by an activation function. This implementation is based on the python package numpy. Try to use numpy functions/matrices wherever possible.\n",
        "\n",
        "We have already provided you with a function that initializes all the properties for you. The MLP is defined by providing the sizes of the layers (including input and output layers) and the activation functions that are applied after each layer. We will use vanilla stochastic gradient descent with mini-batches for weight & bias updates (but for simplicity we will only pass through one instance after another).\n",
        "\n",
        "You can find implementations of activation functions below. They can be used in the forward pass but also provide the derivative if `deriv=True` is provided. See more details the comment in a).\n",
        "\n",
        "Throughout the tasks do not change the arrays that are in the properties but rather use inplace changes if needed. You can e.g. do `self.gradients[0][:] += 1` to increase the value of all gradient entries of the first layer inplace.\n",
        "\n",
        "### a) forward\n",
        "\n",
        "Write a function `forward` that takes an input array that should then be passed through the entire network. All immediate layer results are stored in `y`. Thereby store also the input (raw input) and the output layer.\n",
        "The forward pass goes as follows\n",
        "\n",
        "$\\mathbf{y_0} = \\mathbf{x}$ <br>\n",
        "for k=0.. do<br>\n",
        "_    $z_{k} =  y_{k} W_k  + b_k$ <br>\n",
        "_    $y_{k+1} = g_{k}(z_{k})$\n",
        "\n",
        "Where $W$ is the weight matrix, $b$ is the bias vector and $g$ the activation function at layer $k$. Note that the $y_k$ introduced here are not the same as in the lecture they are more similar to the $u_k$ but still slightly different as we are not treating the activations as their own layer.\n",
        "\n",
        "The variable $z_k$ is not stored but is here for illustration. In general one potentially would have to store it also but we don't need it for the activation functions sigmoid and relu. They have the special property that we can write those  as $g'(x) = G(g(x))$ for some function $G$.\n",
        "\n",
        "### b) back propagation\n",
        "\n",
        "The backpropagation is computed as:\n",
        "\n",
        "\n",
        "$h \\leftarrow \\frac{\\partial L}{\\partial y}$<br>\n",
        "for k = l, l-1,.. 1 do:<br>\n",
        "_ $h \\leftarrow \\frac{\\partial L}{\\partial z_k} = h \\odot g' _{k-1}(z_{k-1}) = h \\odot G_{k-1}(y_k)$<br>\n",
        "_ $\\frac{\\partial L}{\\partial W_{k-1}} = y_{k-1} \\otimes h$<br>\n",
        "_ $\\frac{\\partial L}{\\partial b_{k-1}} = h$<br>\n",
        "_ $h \\leftarrow \\frac{\\partial L}{\\partial y_k} = h \\cdot W_{k-1}$\n",
        "\n",
        "Where $\\odot$ is the element wise product, $\\otimes$ is the outer product and $\\cdot$ the dot product. Assume that a forward step has previosuly happened and has set the values of `results` accordingly. To keep it simple this function is intended for one instance only but we want to employ it in a mini batch scenario. So instead of directly overriding the gradients just add to them.\n",
        "\n",
        "### tanh activation\n",
        "\n",
        "Implement the tanh activation function such that they can also be used in your MLP. See e.g. https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions\n",
        "\n",
        "\n",
        "### d) update\n",
        "\n",
        "Write a function update that updates the weights and biases for all layers based on a mini-batch (given as lists of input arrays `xs` and desired targets `ts` [$\\hat y $ in the lecture]), loss function `loss_func` and and learning rate $\\mu$. It therefore first zeros previous gradients (implement the `zero_grad` function), then does a forward and backwards pass for each training instance and finally updates the weights and biases using the learning rate.\n",
        "\n",
        "This is a simplified process. In pratice all of the instances in the mini-batch would be passed forward and backwards simultanously we don't do it here.\n",
        "\n",
        "### e) Evaluation functions\n",
        "\n",
        "Implement the functions `calc_precision` and `calc_loss` that calculate the classification precision and the *average* loss over the provided examples `xs` and `ts`.\n",
        "\n",
        "\n",
        "### f) Preapring real data\n",
        "Write a function `top_n_words(sentences, n)` that returns a list of the most frequently used words for a corpus. You can assume that the function is tested such that there is no tie.\n",
        "\n",
        "Then write a function `to_hot_encoding(sentences, top_words)` that encodes the document as a numpy array of type bool. It has the same length as `top_words` and indicates the presence or abscence of that particular word by 1 = present, 0 = absent. This is some kind of bag of words model.\n",
        "\n",
        "The sentences are provided as a list of list of tokens. Do *not* do any cleaning on those tokens\n",
        "\n",
        "### g) fiddle with hyper parameters\n",
        "\n",
        "Find hyperparameters\n",
        "```\n",
        "my_seed = 1\n",
        "my_sizes = (128, ...,2)\n",
        "my_activations = (..., sigmoid)\n",
        "my_epochs = 1\n",
        "my_chunks = 1\n",
        "```\n",
        "such that you achieve >90% precision on the training set and >75% precision on the test set using the provided training  loop. You may only use `relu`, `identity`, `tanh` and `sigmoid` activation functions. The training time should not exceed 1min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs47HXKwJU7P"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niD6O5M6JU7Q"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, sizes,activations):\n",
        "        assert len(activations) == (len(sizes)-1)\n",
        "        self.activation_functions = tuple(activations)\n",
        "\n",
        "\n",
        "        # init weights, biases and temporary results\n",
        "        self.weights = tuple(((np.random.random_sample((sizes[i], sizes[i+1]))-0.5) for i in range(len(sizes)-1)))\n",
        "        self.biases = tuple((np.random.random_sample(sizes[i+1])-0.5) for i in range(len(sizes)-1))\n",
        "        self.y = tuple(np.empty(sizes[i]) for i in range(len(sizes)))\n",
        "\n",
        "        # init gradients\n",
        "        self.gradients = tuple((np.zeros(arr.shape) for arr in self.weights))\n",
        "        self.biases_gradients = tuple((np.zeros(arr.shape) for arr in self.biases))\n",
        "\n",
        "\n",
        "    def forward(self, input_arr):\n",
        "        # input_array is a numpy array,\n",
        "        # this function returns nothing\n",
        "\n",
        "        # set y0\n",
        "        for i in range(len(input_arr)):\n",
        "            self.y[0][i] = input_arr[i].astype(float)\n",
        "\n",
        "        for layer in range(0, len(self.y)-1): # layer\n",
        "            vals = self.activation_functions[layer](np.dot(self.y[layer], self.weights[layer]) + self.biases[layer])\n",
        "            for o in range(len(vals)):\n",
        "                self.y[layer+1][o] = vals[o]\n",
        "\n",
        "\n",
        "    def back_prop(self, t, loss_func):\n",
        "        # t : target label == desired form of the final label\n",
        "        # loss_func: a loss function see squared loss below\n",
        "        h = loss_func(self.y[len(self.y)-1], t, deriv=True)\n",
        "        for k in range(len(self.y)-1, 0, -1):\n",
        "            h =  h * self.activation_functions[k-1](self.y[k], deriv=True)\n",
        "            self.gradients[k-1][:] += np.outer(self.y[k-1], h)\n",
        "            self.biases_gradients[k-1][:] += h\n",
        "            h = np.dot(self.weights[k-1], h)\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for i in range(len(self.gradients)):\n",
        "            self.gradients[i][:] = 0\n",
        "            self.biases_gradients[i][:] = 0\n",
        "\n",
        "\n",
        "    def update(self, xs, ts, loss_func, mu):\n",
        "        # xs: list of numpy arrays (inputs)\n",
        "        # ts: list of numpy arrays (desired output)\n",
        "        # loss_func: function, a loss function see squared loss below\n",
        "        # mu: float, learning rate\n",
        "        #assert len(ts) == len(xs)\n",
        "        self.zero_grad()\n",
        "        for i in range(len(xs)):\n",
        "            self.forward(xs[i])\n",
        "            self.back_prop(ts[i], loss_func)\n",
        "        for j in range(len(self.gradients)):\n",
        "            self.weights[j][:] -= mu*self.gradients[j]\n",
        "            self.biases[j][:] -= mu*self.biases_gradients[j]\n",
        "\n",
        "    def calc_precision(self, xs, ts):\n",
        "        # xs: list of numpy arrays (inputs)\n",
        "        # ts: list of numpy arrays (desired output)\n",
        "        right = 0\n",
        "        for i in range(len(xs)):\n",
        "            self.forward(xs[i])\n",
        "            if (ts[i][self.y[len(self.y)-1].argmax()]):\n",
        "                right += 1\n",
        "        return right/len(xs)\n",
        "\n",
        "    def calc_loss(self, xs, ts, loss_func):\n",
        "        # xs: list of numpy arrays (inputs)\n",
        "        # ts: list of numpy arrays (desired output)\n",
        "        # loss_func: function, a loss function see squared loss below\n",
        "        #arr = np.zeros(len(xs))\n",
        "        loss = 0\n",
        "        for i in range(len(xs)):\n",
        "            self.forward(xs[i])\n",
        "            #arr[i] = loss_func(self.y[len(self.y)-1], ts[i])\n",
        "            loss += loss_func(self.y[len(self.y)-1], ts[i])\n",
        "        return loss / len(xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB2CFj47JU7Q"
      },
      "outputs": [],
      "source": [
        "# activation functions\n",
        "# if deriv=True they act the function capital G as explained above\n",
        "def identity(x, deriv=False):\n",
        "    if deriv == True:\n",
        "        return 1\n",
        "    return x\n",
        "\n",
        "def relu(x, deriv=False):\n",
        "    if deriv == True:\n",
        "        out = np.zeros(x.shape)\n",
        "        out[x>0]=x[x>0]\n",
        "        return out\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def sigmoid(x, deriv=False):\n",
        "    if deriv == True:\n",
        "        return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x, deriv=False):\n",
        "    # print(tanh(np.array([1,2,3]))) [0.76159416 0.96402758 0.99505475]\n",
        "    # print(tanh(np.array([1,2,3]), deriv=True)) [ 0 -3 -8]\n",
        "    if deriv == True:\n",
        "        return 1.0 - x**2\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJQ1ek0XJU7Q",
        "outputId": "de035da7-0506-46ee-ea51-4906930c8a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.76159416 0.96402758 0.99505475]\n",
            "[ 0. -3. -8.]\n"
          ]
        }
      ],
      "source": [
        "print(tanh(np.array([1,2,3])))\n",
        "print(tanh(np.array([1,2,3]), deriv=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfOAgGdXJU7R"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "def squared_loss(x, t, deriv=False):\n",
        "    if deriv:\n",
        "        return x - t\n",
        "    return 0.5 * np.sum((np.square(x-t)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RrolJeFJU7R",
        "outputId": "f9f30363-bd0c-4122-cc64-8f5d14591386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0.]\n",
            "[0.         0.28896105 0.4080556  0.43338515]\n",
            "[0.         0.03587933 0.        ]\n",
            "[0.48869641 0.5991624 ]\n"
          ]
        }
      ],
      "source": [
        "in1 = np.array([0,0,1,0,0], dtype=bool)\n",
        "np.random.seed(1)\n",
        "\n",
        "activs = [relu, relu, sigmoid]\n",
        "myNN = MLP((5,4,3,2), activs)\n",
        "\n",
        "\n",
        "myNN.forward(in1)\n",
        "print(myNN.y[0])\n",
        "print(myNN.y[1])\n",
        "print(myNN.y[2])\n",
        "print(myNN.y[-1])\n",
        "\n",
        "#[0. 0. 1. 0. 0.]\n",
        "#[0.         0.28896105 0.4080556  0.43338515]\n",
        "#[0.         0.03587933 0.        ]\n",
        "#[0.48869641 0.5991624 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdDQ-Bn8JU7S",
        "outputId": "b4dea3ab-14bc-4db0-cd6b-2f768e97265d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 0. 1. 0. 0.]\n",
            "[0.         0.50928554 0.         0.23571773]\n",
            "[0.         0.38629211 0.        ]\n",
            "[0.50550331 0.58354196]\n"
          ]
        }
      ],
      "source": [
        "in2 = np.array([1,0,1,0,0], dtype=bool)\n",
        "myNN.forward(in2)\n",
        "print(myNN.y[0])\n",
        "print(myNN.y[1])\n",
        "print(myNN.y[2])\n",
        "print(myNN.y[-1])\n",
        "#[1. 0. 1. 0. 0.]\n",
        "#[0.         0.50928554 0.         0.23571773]\n",
        "#[0.         0.38629211 0.        ]\n",
        "#[0.50550331 0.58354196]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMUOeYI3JU7S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkad5gz1JU7S",
        "outputId": "d4516bff-a1df-4512-9db7-78d653fad8b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
            "       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
            "       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
            "       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
            "       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
            "       [ 0.19232262,  0.37638915,  0.39460666],\n",
            "       [-0.41495579, -0.46094522, -0.33016958],\n",
            "       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
            "       [ 0.19187711, -0.18448437],\n",
            "       [ 0.18650093,  0.33462567]]))\n",
            "(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
            "(array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        , -0.00019926,  0.00034459,  0.00031891],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        , -0.00052939,  0.        ],\n",
            "       [ 0.        , -0.00074758,  0.        ],\n",
            "       [ 0.        , -0.00079398,  0.        ]]), array([[ 0.        ,  0.        ],\n",
            "       [-0.00458396,  0.005163  ],\n",
            "       [ 0.        ,  0.        ]]))\n",
            "(array([ 0.        , -0.00019926,  0.00034459,  0.00031891]), array([ 0.        , -0.00183205,  0.        ]), array([-0.12776057,  0.14389893]))\n"
          ]
        }
      ],
      "source": [
        "myNN.zero_grad()\n",
        "myNN.forward(in1)\n",
        "myNN.back_prop(np.array([1,0]), squared_loss)\n",
        "\n",
        "# untouched\n",
        "print(myNN.weights)\n",
        "print(myNN.biases)\n",
        "\n",
        "# changed\n",
        "print(myNN.gradients)\n",
        "print(myNN.biases_gradients)\n",
        "\n",
        "# (array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
        "#       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
        "#       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
        "#       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
        "#       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]),\n",
        "#        array([[ 0.30074457,  0.46826158, -0.18657582],\n",
        "#       [ 0.19232262,  0.37638915,  0.39460666],\n",
        "#       [-0.41495579, -0.46094522, -0.33016958],\n",
        "#       [ 0.3781425 , -0.40165317, -0.07889237]]),\n",
        "#        array([[ 0.45788953,  0.03316528],\n",
        "#       [ 0.19187711, -0.18448437],\n",
        "#       [ 0.18650093,  0.33462567]]))\n",
        "#(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
        "#(array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        , -0.00019926,  0.00034459,  0.00031891],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        , -0.00052939,  0.        ],\n",
        "#       [ 0.        , -0.00074758,  0.        ],\n",
        "#       [ 0.        , -0.00079398,  0.        ]]), array([[ 0.        ,  0.        ],\n",
        "#       [-0.00458396,  0.005163  ],\n",
        "#       [ 0.        ,  0.        ]]))\n",
        "#(array([ 0.        , -0.00019926,  0.00034459,  0.00031891]), array([ 0.        , -0.00183205,  0.        ]), array([-0.12776057,  0.14389893]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXdbu0xOJU7S",
        "outputId": "e0be143f-61e4-4c2c-a820-d711754271d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
            "       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
            "       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
            "       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
            "       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
            "       [ 0.19232262,  0.37638915,  0.39460666],\n",
            "       [-0.41495579, -0.46094522, -0.33016958],\n",
            "       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
            "       [ 0.19187711, -0.18448437],\n",
            "       [ 0.18650093,  0.33462567]]))\n",
            "(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
            "(array([[ 0.        , -0.00738705,  0.        ,  0.00364851],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        , -0.00738705,  0.        ,  0.00364851],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        , -0.01962609,  0.        ],\n",
            "       [ 0.        ,  0.        ,  0.        ],\n",
            "       [ 0.        , -0.00908374,  0.        ]]), array([[ 0.        ,  0.        ],\n",
            "       [-0.09549851,  0.10956233],\n",
            "       [ 0.        ,  0.        ]]))\n",
            "(array([ 0.        , -0.00738705,  0.        ,  0.00364851]), array([ 0.        , -0.03853652,  0.        ]), array([-0.24721839,  0.2836256 ]))\n"
          ]
        }
      ],
      "source": [
        "# accumulates gradients over two examples\n",
        "myNN.zero_grad()\n",
        "myNN.forward(in2)\n",
        "myNN.back_prop(np.array([1,0]), squared_loss)\n",
        "myNN.forward(in2)\n",
        "myNN.back_prop(np.array([1,0]), squared_loss)\n",
        "\n",
        "\n",
        "# untouched\n",
        "print(myNN.weights)\n",
        "print(myNN.biases)\n",
        "\n",
        "# changed\n",
        "print(myNN.gradients)\n",
        "print(myNN.biases_gradients)\n",
        "\n",
        "#(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
        "#       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
        "#       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
        "#       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
        "#       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
        "#       [ 0.19232262,  0.37638915,  0.39460666],\n",
        "#       [-0.41495579, -0.46094522, -0.33016958],\n",
        "#       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
        "#       [ 0.19187711, -0.18448437],\n",
        "#       [ 0.18650093,  0.33462567]]))\n",
        "#(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
        "#(array([[ 0.        , -0.00738705,  0.        ,  0.00364851],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        , -0.00738705,  0.        ,  0.00364851],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        , -0.01962609,  0.        ],\n",
        "#       [ 0.        ,  0.        ,  0.        ],\n",
        "#       [ 0.        , -0.00908374,  0.        ]]), array([[ 0.        ,  0.        ],\n",
        "#       [-0.09549851,  0.10956233],\n",
        "#       [ 0.        ,  0.        ]]))\n",
        "#(array([ 0.        , -0.00738705,  0.        ,  0.00364851]), array([ 0.        , -0.03853652,  0.        ]), array([-0.24721839,  0.2836256 ]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glQQhc98JU7T"
      },
      "outputs": [],
      "source": [
        "# a function that generates some dummy training dataset\n",
        "def generate_training(n):\n",
        "    xs=[]\n",
        "    ts=[]\n",
        "    for _ in range(n):\n",
        "        x = np.random.random_sample(10)>0.5\n",
        "        t = (x[1] and x[2]) or x[3]\n",
        "        xs.append(x)\n",
        "        ts.append(np.array([t,~t]))\n",
        "    return xs, ts\n",
        "    return np.array(xs), np.array(ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aep2lR7bJU7T"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "xs, ts = generate_training(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "CiXfFGfzJU7T",
        "outputId": "a491f71e-0dc1-4763-f3cf-190528935375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0.24257539789489851 0.634\n",
            "# 0.2411544143560101 0.634\n",
            "# 0.23984965535059738 0.634\n",
            "# 0.23882087470347274 0.634\n",
            "# 0.23843002829715368 0.634\n",
            "# 0.2382914663918927 0.634\n",
            "# 0.23764527725780307 0.634\n",
            "# 0.23666737755900943 0.634\n",
            "# 0.23620884986400037 0.634\n",
            "# 0.2365034031129388 0.634\n",
            "# 0.23616300372820928 0.634\n",
            "# 0.23502874505528465 0.634\n",
            "# 0.23503090772220395 0.634\n",
            "# 0.23429557276477497 0.634\n",
            "# 0.23407573863293182 0.634\n",
            "# 0.2339776485620543 0.634\n",
            "# 0.23348187532872453 0.634\n",
            "# 0.23290833656298032 0.634\n",
            "# 0.23273709472388385 0.634\n",
            "# 0.2325444855821718 0.634\n",
            "# 0.23267973317482651 0.634\n",
            "# 0.23200642490995707 0.634\n",
            "# 0.23170660058816217 0.634\n",
            "# 0.2314284286110461 0.634\n",
            "# 0.2310843786009857 0.634\n",
            "# 0.2308711195328345 0.634\n",
            "# 0.23047087788541018 0.634\n",
            "# 0.23078563724882506 0.634\n",
            "# 0.23010159343696798 0.634\n",
            "# 0.22980399458119968 0.634\n",
            "# 0.2297798677979091 0.634\n",
            "# 0.2293841765404658 0.634\n",
            "# 0.2291655512063339 0.634\n",
            "# 0.22870691228278373 0.634\n",
            "# 0.22817010393724937 0.634\n",
            "# 0.22771610716266258 0.634\n",
            "# 0.22735622344120002 0.634\n",
            "# 0.2268242701172851 0.634\n",
            "# 0.22650100063083706 0.634\n",
            "# 0.22550104232382934 0.634\n",
            "# 0.22509370708150808 0.634\n",
            "# 0.22467802344347212 0.634\n",
            "# 0.2242107007448516 0.634\n",
            "# 0.2233116482328667 0.634\n",
            "# 0.22265497659346642 0.634\n",
            "# 0.22178494678736085 0.634\n",
            "# 0.2213379424960512 0.634\n",
            "# 0.21990383266003788 0.634\n",
            "# 0.21838496173099076 0.634\n",
            "# 0.21753192292293697 0.634\n",
            "# 0.21669299576563952 0.634\n",
            "# 0.21663406404375593 0.634\n",
            "# 0.2140600877981478 0.634\n",
            "# 0.21325662957467928 0.634\n",
            "# 0.2128202380539361 0.634\n",
            "# 0.21274278318993378 0.634\n",
            "# 0.21016631292648913 0.634\n",
            "# 0.20932144343716885 0.634\n",
            "# 0.21282663190076198 0.634\n",
            "# 0.21085248695856354 0.634\n",
            "# 0.20919298956487534 0.634\n",
            "# 0.2040259930808181 0.634\n",
            "# 0.20830483920311055 0.634\n",
            "# 0.20110187393197654 0.634\n",
            "# 0.19921054775576252 0.634\n",
            "# 0.19614748720352942 0.634\n",
            "# 0.19579048879561475 0.634\n",
            "# 0.19344528532212424 0.634\n",
            "# 0.20045251332451106 0.634\n",
            "# 0.1882574769491476 0.634\n",
            "# 0.18576924889914845 0.634\n",
            "# 0.18411287775658522 0.634\n",
            "# 0.1830261558425337 0.634\n",
            "# 0.1845476342922807 0.634\n",
            "# 0.17626642470559314 0.635\n",
            "# 0.17469065098247075 0.679\n",
            "# 0.1719378298327115 0.661\n",
            "# 0.17605269372506274 0.637\n",
            "# 0.19701745527854844 0.634\n",
            "# 0.1608919682268122 0.716\n",
            "# 0.16342926706599922 0.651\n",
            "# 0.19137490242816632 0.938\n",
            "# 0.16993445388782416 0.662\n",
            "# 0.17335348387678515 0.65\n",
            "# 0.15162184456043237 0.682\n",
            "# 0.14235256482453104 0.807\n",
            "# 0.1512034857104882 0.69\n",
            "# 0.15666428550872574 0.681\n",
            "# 0.25674954416271745 0.634\n",
            "# 0.15486582586192132 0.95\n",
            "# 0.19636555320933277 0.652\n",
            "# 0.18016546272625983 0.84\n",
            "# 0.12896755949196498 0.742\n",
            "# 0.13573770765700857 0.862\n",
            "# 0.09399810441538302 0.921\n",
            "# 0.09851463017361556 0.903\n",
            "# 0.11583907110986205 0.943\n",
            "# 0.3263687991427449 0.634\n",
            "# 0.2195528529634276 0.681\n",
            "# 0.22320714836930242 0.676\n",
            "# 0.08403512410215165 0.976\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "myNN = MLP((10,8,4, 2), activs)\n",
        "print(\"#\", myNN.calc_loss(xs,ts, squared_loss), myNN.calc_precision(xs, ts))\n",
        "\n",
        "for x_chunk, t_chunk in zip(np.array_split(xs,100), np.array_split(ts,100)):\n",
        "    myNN.update(x_chunk, t_chunk, squared_loss, 0.1)\n",
        "    # evaluate NN on entire dataset\n",
        "    print(\"#\", myNN.calc_loss(xs,ts, squared_loss), myNN.calc_precision(xs, ts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsdJT8O3JU7T"
      },
      "outputs": [],
      "source": [
        "# output from previous cell\n",
        "# 0.24257539789489851 0.634\n",
        "# 0.2411544143560101 0.634\n",
        "# 0.23984965535059738 0.634\n",
        "# 0.23882087470347274 0.634\n",
        "# 0.23843002829715368 0.634\n",
        "# 0.2382914663918927 0.634\n",
        "# 0.23764527725780307 0.634\n",
        "# 0.23666737755900943 0.634\n",
        "# 0.23620884986400037 0.634\n",
        "# 0.23650340311293885 0.634\n",
        "# 0.23616300372820928 0.634\n",
        "# 0.23502874505528465 0.634\n",
        "# 0.23503090772220395 0.634\n",
        "# 0.23429557276477497 0.634\n",
        "# 0.23407573863293182 0.634\n",
        "# 0.2339776485620543 0.634\n",
        "# 0.23348187532872453 0.634\n",
        "# 0.23290833656298032 0.634\n",
        "# 0.23273709472388385 0.634\n",
        "# 0.2325444855821718 0.634\n",
        "# 0.23267973317482651 0.634\n",
        "# 0.23200642490995707 0.634\n",
        "# 0.23170660058816217 0.634\n",
        "# 0.2314284286110461 0.634\n",
        "# 0.2310843786009857 0.634\n",
        "# 0.2308711195328345 0.634\n",
        "# 0.23047087788541018 0.634\n",
        "# 0.23078563724882506 0.634\n",
        "# 0.23010159343696798 0.634\n",
        "# 0.22980399458119968 0.634\n",
        "# 0.2297798677979091 0.634\n",
        "# 0.2293841765404658 0.634\n",
        "# 0.2291655512063339 0.634\n",
        "# 0.22870691228278373 0.634\n",
        "# 0.22817010393724937 0.634\n",
        "# 0.22771610716266258 0.634\n",
        "# 0.22735622344120002 0.634\n",
        "# 0.2268242701172851 0.634\n",
        "# 0.22650100063083706 0.634\n",
        "# 0.22550104232382934 0.634\n",
        "# 0.22509370708150808 0.634\n",
        "# 0.22467802344347212 0.634\n",
        "# 0.2242107007448516 0.634\n",
        "# 0.2233116482328667 0.634\n",
        "# 0.22265497659346642 0.634\n",
        "# 0.22178494678736085 0.634\n",
        "# 0.2213379424960512 0.634\n",
        "# 0.21990383266003793 0.634\n",
        "# 0.21838496173099078 0.634\n",
        "# 0.21753192292293697 0.634\n",
        "# 0.21669299576563952 0.634\n",
        "# 0.21663406404375596 0.634\n",
        "# 0.2140600877981478 0.634\n",
        "# 0.21325662957467928 0.634\n",
        "# 0.21282023805393604 0.634\n",
        "# 0.21274278318993375 0.634\n",
        "# 0.21016631292648913 0.634\n",
        "# 0.20932144343716885 0.634\n",
        "# 0.21282663190076195 0.634\n",
        "# 0.21085248695856348 0.634\n",
        "# 0.2091929895648754 0.634\n",
        "# 0.2040259930808181 0.634\n",
        "# 0.20830483920311055 0.634\n",
        "# 0.20110187393197654 0.634\n",
        "# 0.19921054775576258 0.634\n",
        "# 0.19614748720352942 0.634\n",
        "# 0.19579048879561475 0.634\n",
        "# 0.19344528532212424 0.634\n",
        "# 0.20045251332451106 0.634\n",
        "# 0.18825747694914757 0.634\n",
        "# 0.18576924889914842 0.634\n",
        "# 0.18411287775658522 0.634\n",
        "# 0.18302615584253373 0.634\n",
        "# 0.18454763429228072 0.634\n",
        "# 0.17626642470559317 0.635\n",
        "# 0.17469065098247072 0.679\n",
        "# 0.1719378298327115 0.661\n",
        "# 0.17605269372506277 0.637\n",
        "# 0.19701745527854836 0.634\n",
        "# 0.1608919682268122 0.716\n",
        "# 0.16342926706599922 0.651\n",
        "# 0.19137490242816632 0.938\n",
        "# 0.16993445388782424 0.662\n",
        "# 0.17335348387678517 0.65\n",
        "# 0.15162184456043243 0.682\n",
        "# 0.14235256482453107 0.807\n",
        "# 0.1512034857104882 0.69\n",
        "# 0.15666428550872563 0.681\n",
        "# 0.2567495441627176 0.634\n",
        "# 0.15486582586192096 0.95\n",
        "# 0.19636555320933272 0.652\n",
        "# 0.18016546272626016 0.84\n",
        "# 0.128967559491965 0.742\n",
        "# 0.13573770765700863 0.862\n",
        "# 0.09399810441538305 0.921\n",
        "# 0.09851463017361554 0.903\n",
        "# 0.11583907110986205 0.943\n",
        "# 0.32636879914274497 0.634\n",
        "# 0.2195528529634276 0.681\n",
        "# 0.2232071483693024 0.676\n",
        "# 0.08403512410215158 0.976"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-4WyBO0JU7T"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "X=None\n",
        "with open('news_sports_train_X.p', 'rb') as f:\n",
        "    X=pickle.load(f)\n",
        "\n",
        "X_test=None\n",
        "with open('news_sports_test_X.p', 'rb') as f:\n",
        "    X_test=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acB8EiwjJU7U"
      },
      "outputs": [],
      "source": [
        "y=None\n",
        "with open('news_sports_train_y.p', 'rb') as f:\n",
        "    y=pickle.load(f)\n",
        "\n",
        "y_test=None\n",
        "with open('news_sports_test_y.p', 'rb') as f:\n",
        "    y_test=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np8blF8jJU7U"
      },
      "outputs": [],
      "source": [
        "# prepare targets\n",
        "def get_ts(labels):\n",
        "    ts = []\n",
        "    for label in labels:\n",
        "        l = bool(label)\n",
        "        ts.append(np.array([not l, l], dtype=bool))\n",
        "    return ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsMosfOVJU7U"
      },
      "outputs": [],
      "source": [
        "ts = get_ts(y)\n",
        "ts_test = get_ts(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLSC9pH7JU7U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkl7BgQJJU7U"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdHVXA2xJU7U"
      },
      "outputs": [],
      "source": [
        "def top_n_words(sentences,n):\n",
        "    counts = Counter()\n",
        "    for sen in range(len(sentences)):\n",
        "        for word in range(len(sentences[sen])):\n",
        "            counts[sentences[sen][word]] += 1\n",
        "    return [count[0] for count in counts.most_common(n)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_AMPWkLJU7U",
        "outputId": "1901cf8c-41ef-440f-aeec-b8a8e6263c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[',', '--', 'the', '>', '.', ':', ')', '(', 'to', 'a']"
            ]
          },
          "execution_count": 353,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_n_words(X,10)\n",
        "# [',', '--', 'the', '>', '.', ':', ')', '(', 'to', 'a']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SoRjVVdJU7U"
      },
      "outputs": [],
      "source": [
        "def to_hot_encoding(sentences, top_words):\n",
        "    encoding = []\n",
        "    for sen in range(len(sentences)):\n",
        "        sen_encoding = np.zeros(len(top_words), dtype=bool)\n",
        "        for tw in range(len(top_words)):\n",
        "            if top_words[tw] in sentences[sen]:\n",
        "                sen_encoding[tw] = True;\n",
        "        encoding.append(sen_encoding)\n",
        "    return encoding\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP5M3v9vJU7V",
        "outputId": "74982739-2aa9-46c3-8c6b-4be2ea8036fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        False]),\n",
              " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True])]"
            ]
          },
          "execution_count": 355,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_hot_encoding(X[:2], top_n_words(X,10))\n",
        "#[array([ True,  True,  True,  True,  True,  True,  True,  True,  True, False]),\n",
        "# array([ True,  True,  True,  True,  True,  True,  True,  True,  True, True])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj0RcX3CJU7V"
      },
      "outputs": [],
      "source": [
        "top_words = top_n_words(X, 128)\n",
        "arrs = to_hot_encoding(X, top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56ZvLao4JU7V"
      },
      "outputs": [],
      "source": [
        "arrs_test = to_hot_encoding(X_test, top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qalmf3FJU7V"
      },
      "outputs": [],
      "source": [
        "my_seed = 2\n",
        "my_sizes = (128,64,32,2)\n",
        "my_activations = (relu, tanh, sigmoid) #(relu, tanh, sigmoid)\n",
        "my_epochs = 17 #32\n",
        "my_chunks = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwQj42poJU7V",
        "outputId": "048c0055-f083-4778-9fa0-7bc39096b2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "# 0.23262962330031403 0.6541274817136886\n",
            "# test 0.24795578657593675 0.5841708542713567\n",
            "1\n",
            "# 0.20212388102198167 0.7053291536050157\n",
            "# test 0.2239855720401033 0.6369346733668342\n",
            "2\n",
            "# 0.16870554407840171 0.7596656217345873\n",
            "# test 0.20049376097426402 0.6934673366834171\n",
            "3\n",
            "# 0.14183293395416852 0.8066875653082549\n",
            "# test 0.18413570774034732 0.7173366834170855\n",
            "4\n",
            "# 0.12625565577064038 0.8286311389759665\n",
            "# test 0.17738280431613504 0.7298994974874372\n",
            "5\n",
            "# 0.10972468779135945 0.8557993730407524\n",
            "# test 0.1716883095005804 0.742462311557789\n",
            "6\n",
            "# 0.11389447971048172 0.8537095088819227\n",
            "# test 0.17714632810174072 0.7361809045226131\n",
            "7\n",
            "# 0.15790282348608978 0.7565308254963428\n",
            "# test 0.21765937976857913 0.6834170854271356\n",
            "8\n",
            "# 0.15780070338777324 0.7617554858934169\n",
            "# test 0.21914941911865682 0.6909547738693468\n",
            "9\n",
            "# 0.1288806053979431 0.8234064785788924\n",
            "# test 0.19840373616142393 0.7047738693467337\n",
            "10\n",
            "# 0.09646304253826962 0.8766980146290491\n",
            "# test 0.17442066150653404 0.7437185929648241\n",
            "11\n",
            "# 0.08202859214080102 0.8944618599791013\n",
            "# test 0.17176012013911462 0.7550251256281407\n",
            "12\n",
            "# 0.08205510316037448 0.8944618599791013\n",
            "# test 0.17246190619556476 0.7550251256281407\n",
            "13\n",
            "# 0.16079357962869953 0.754440961337513\n",
            "# test 0.23507845485814474 0.6846733668341709\n",
            "14\n",
            "# 0.07169763059290758 0.9080459770114943\n",
            "# test 0.16793754544299652 0.7587939698492462\n",
            "15\n",
            "# 0.07835772160292682 0.8913270637408568\n",
            "# test 0.18367022367497585 0.7386934673366834\n",
            "16\n",
            "# 0.07809192322408028 0.9038662486938349\n",
            "# test 0.1751513383056301 0.7525125628140703\n"
          ]
        }
      ],
      "source": [
        "# training loop for task g\n",
        "\n",
        "np.random.seed(my_seed)\n",
        "myNN = MLP(my_sizes, my_activations)\n",
        "n_chunks = my_chunks\n",
        "n_epochs = my_epochs\n",
        "for i in range(n_epochs):\n",
        "    for x_chunk, t_chunk in zip(np.array_split(arrs,n_chunks), np.array_split(ts,n_chunks)):\n",
        "        myNN.update(x_chunk, t_chunk, squared_loss, 0.01)\n",
        "        # evaluate NN on entire dataset\n",
        "    print(i)\n",
        "    print(\"#\", myNN.calc_loss(arrs,ts, squared_loss), myNN.calc_precision(arrs, ts))\n",
        "    print(\"# test\", myNN.calc_loss(arrs_test,ts_test, squared_loss), myNN.calc_precision(arrs_test, ts_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuemXY2JJU7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-UDCgAOJU7V"
      },
      "source": [
        "# Task 2: Viterbi algorithm\n",
        "Write a function that implements the Viterbi algorithm using beamsearch. The function takes as input a sentence, state transition probability and word emission probability. State transition and word emission probabilities are provided below.\n",
        "\n",
        "For each step the beam is expanded with potential pos tags. The most promising examples are then retained.\n",
        "\n",
        "The function returns the beam after the final word has been processed.\n",
        "The beam is a list of 2-tuples. The first entry is the probability of that sentence, the second entry is a tuple that contains all the pos-tags that were assigned to the words.\n",
        "\n",
        "Hint: you may want to use the heapq https://docs.python.org/3/library/heapq.html functions for your beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX_Kw8D6JU7W"
      },
      "outputs": [],
      "source": [
        "def Viterbi(trans_dict, word_emission_prob, sentence, beam_width):\n",
        "    res = []\n",
        "    candidates = []\n",
        "    trans_keys = get_keys(trans_dict, '<s>')\n",
        "    prob_keys = get_keys(word_emission_prob, sentence[0])\n",
        "    for tk in trans_keys:\n",
        "        pk = (sentence[0], tk[1])\n",
        "        if (pk in prob_keys):\n",
        "            candidates.append((trans_dict[tk] * word_emission_prob[pk], [pk[1]]))\n",
        "    res = sorted(candidates, key=lambda x: (x[0]))[:beam_width]\n",
        "\n",
        "    for word in range(len(sentence) - 1):\n",
        "        candidates = []\n",
        "\n",
        "        prob_keys = get_keys(word_emission_prob, sentence[word+1])\n",
        "        for path in res:\n",
        "            trans_keys = get_keys(trans_dict, path[1][len(path[1]) - 1])\n",
        "            for tk in trans_keys:\n",
        "                pk = (sentence[word+1], tk[1])\n",
        "                if (pk in prob_keys):\n",
        "                    candidates.append((path[0] * trans_dict[tk] * word_emission_prob[pk], path[1] + [tk[1]]))\n",
        "        res = sorted(candidates, key=lambda x: (-x[0]))[:beam_width]\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_keys(dictionary, key):\n",
        "    return list(filter(lambda k: k[0] == key, dictionary.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd-xL_10JU7W"
      },
      "outputs": [],
      "source": [
        "state_trans_prob = {('<s>','NNP'):0.2767,('<s>','MD'):0.006,('<s>','VB'):0.0031,('<s>','JJ'):0.0453,('<s>','NN'):0.0449,\n",
        "                   ('<s>','RB'):0.0510,('<s>','DT'):0.2026,\n",
        "                   ('NNP','NNP'):0.3777,('NNP','MD'):0.0110,('NNP','VB'):0.0009,('NNP','JJ'):0.0084,('NNP','NN'):0.0584,\n",
        "                   ('NNP','RB'):0.0090,('NNP','DT'):0.0025,\n",
        "                   ('MD','NNP'):0.0008,('MD','MD'):0.0002,('MD','VB'):0.7968,('MD','JJ'):0.0005,('MD','NN'):0.0008,\n",
        "                   ('MD','RB'):0.1698,('MD','DT'):0.0041,\n",
        "                   ('VB','NNP'):0.0322,('VB','MD'):0.0005,('VB','VB'):0.0050,('VB','JJ'):0.0837,('VB','NN'):0.0615,\n",
        "                   ('VB','RB'):0.0514,('VB','DT'):0.2231,\n",
        "                   ('JJ','NNP'):0.0366,('JJ','MD'):0.0004,('JJ','VB'):0.0001,('JJ','JJ'):0.0733,('JJ','NN'):0.4509,\n",
        "                   ('JJ','RB'):0.0036,('JJ','DT'):0.0036,\n",
        "                   ('NN','NNP'):0.0096,('NN','MD'):0.0176,('NN','VB'):0.0014,('NN','JJ'):0.0086,('NN','NN'):0.1216,\n",
        "                   ('NN','RB'):0.0177,('NN','DT'):0.0068,\n",
        "                   ('RB','NNP'):0.0068,('RB','MD'):0.0102,('RB','VB'):0.1011,('RB','JJ'):0.1012,('RB','NN'):0.0120,\n",
        "                   ('RB','RB'):0.0728,('RB','DT'):0.0479,\n",
        "                   ('DT','NNP'):0.1147,('DT','MD'):0.0021,('DT','VB'):0.0002,('DT','JJ'):0.2157,('DT','NN'):0.4744,\n",
        "                   ('DT','RB'):0.0102,('DT','DT'):0.0017}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CRN0H3oJU7W"
      },
      "outputs": [],
      "source": [
        "word_emission_prob = {('Janet','NNP'):0.000032, ('will','MD'):0.308431,('will','VB'):0.000028,('will','NN'):0.0002,\n",
        "                     ('back','VB'):0.000672,('back','JJ'):0.00034,('back','NN'):0.000223,('back','RB'):0.010446,\n",
        "                     ('the','NNP'):0.000048,('the','DT'):0.506099,('bill','VB'):0.000028,('bill','NN'):0.002337}\n",
        "# assume the missing entries are 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eIp_cU5JU7W"
      },
      "outputs": [],
      "source": [
        "sentence = ['Janet', 'will', 'back', 'the', 'bill']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-2jxk33JU7W",
        "outputId": "4ac12ebe-f41e-4109-8f6d-33ca84bfb02a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1.4320953590187012e-15, ['NNP', 'MD', 'RB', 'DT', 'NN'])]"
            ]
          },
          "execution_count": 364,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Viterbi(state_trans_prob, word_emission_prob, sentence, 1)\n",
        "\n",
        "# [(1.4320953590187012e-15, ('NNP', 'MD', 'RB', 'DT', 'NN'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjEF3Z8EJU7X",
        "outputId": "e1b8b9dc-a03d-4aa4-a6e1-50a476b22032"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(2.013570710221386e-15, ['NNP', 'MD', 'VB', 'DT', 'NN']),\n",
              " (1.4320953590187012e-15, ['NNP', 'MD', 'RB', 'DT', 'NN'])]"
            ]
          },
          "execution_count": 365,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Viterbi(state_trans_prob, word_emission_prob, sentence, 2)\n",
        "\n",
        "# [(2.013570710221386e-15, ('NNP', 'MD', 'VB', 'DT', 'NN')),\n",
        "# (1.4320953590187012e-15, ('NNP', 'MD', 'RB', 'DT', 'NN'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGRhhoL4JU7X",
        "outputId": "a82f115a-2f0d-4a8d-a1ea-2571a1207a8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(2.013570710221386e-15, ['NNP', 'MD', 'VB', 'DT', 'NN']),\n",
              " (1.4320953590187012e-15, ['NNP', 'MD', 'RB', 'DT', 'NN']),\n",
              " (5.139248921926215e-19, ['NNP', 'NN', 'RB', 'DT', 'NN'])]"
            ]
          },
          "execution_count": 366,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Viterbi(state_trans_prob, word_emission_prob, sentence, 3)\n",
        "\n",
        "#[(2.013570710221386e-15, (''NNP', 'MD', 'VB', 'DT', 'NN')),\n",
        "# (1.4320953590187012e-15, ('NNP', 'MD', 'RB', 'DT', 'NN')),\n",
        "# (5.139248921926215e-19, ('NNP', 'NN', 'RB', 'DT', 'NN'))]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}